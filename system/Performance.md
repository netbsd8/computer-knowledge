# Metrics
## CPU utilization
- per min -> per sec -> hitmap (each col a second, each row may be several tens of micrsec, color dense show CPU usage at that time period)
    - perf: Linux profiler (sampling)
      - perf record -F99 -a
    - CPU flame graph (via perf): a visualization of profiled stack traces and what are the precentage that the different code functions are doing
      - x-axis: population of the samples; y-axis: function
      - [flame graph](../pictures/CPU-flame-graph-implementation.png)

- Root Cause
  - Basic Steps:
    - [uptime](../pictures/uptime_output.png)
    - [top](../pictures/top_output.png)
    - [vmstat](../pictures/vmstat_output.png)
    - [iostat](../pictures/iostat_output.png)
    - [free](../pictures/free_output.png)
    - [strace](../pictures/strace_output.png)
    - [tcpdump](../pictures/tcpdump_output.png)
    - [nstat](../pictures/nstat_output.png)
    - [slabtop](../pictures/slabtop_output.png)
    - [pcstat](../pictures/pcstat_output.png)
    - [docker stats](../pictures/docker_stats_output.png)
    - [showboost](../pictures/showboost_output.png)
    - [other static performance tuning tools](../pictures/static_performance_tunning_tools.png)
  - Observability
    - [tools](../pictures/Linux_components_performance_tools.png) vs htop with colors
      - sampling, so it may miss short-run processes; using atop instead
  - Methodology
    - Service Level:
      - Resource Analysis
      - RED method
        - Request Rate
        - Errors
        - Duration
      - Metric and event correlations
        - tracing
      - Latency Drilldowns
    - Instance Level:
      - Log Analysis
      - Micro-benchmarking
      - Drill-down analysis
      - Workload Characterization
        - Who: which pids, programs, users
        - Why: code paths, context
        - What: CPU instructions, cycles
        - How: changing over time
      - USE method
        - Utilization
        - Saturation --> queue
          - if CPU is saturated (high CPU utilization, there may be competition for the CPU caches)
        - Errors
      - [linux perf analysis in 60s](../pictures/Linux_perf_analysis_in_60s.png)
        - uptime --> load averages
        - dmesg -T | tail --> kernel errors
        - vmstat 1 --> overall stats by time
        - mpstat -P ALL 1 --> CPU balance
        - pidstat 1 --> process usage
        - iostat -xz 1 --> disk I/O
        - free -m --> memory usage
        - sar -n DEV 1 --> network I/O
        - sar -n TCP, ETCP 1 --> TCP stats
        - top --> check overview
      - [Linux performance Tool](../pictures/Linux_components_performance_tools.png)
        - Container ware tool (cgroup supporting)
        - Statistics
          - vmstat, pidstat, iostat, 
            - [mpstat](../pictures/mpstat_output.png), 
            - [pmcarch](../pictures/pmcarch_output.png) --> intel performance counters
              - IPC metrics --> it should be tuned to >= 1.5
              - LLC: L3 cache hit rate
              - [context switch](../pictures/perf_context_switch.png): 
              - [cpudist](../pictures/bpf_cpudist_process.png)
                - high value will clear the L1 cache, put more pressure on LC cache and drive the hit rate down
                - high value will prevent application from having enough time to warm up the CPU cache
            - sar
        - Profiling
          - CPU flame graph
        - Tracing
          - [tracing stack](../pictures/Linux_tracing_stack.png)
          -  ftrace
          -  perf
             -  iolatency (hardware)
             -  iosnoop (application)
             -  example: perf record -e block:block_rq_issue --filter rwbs ~ "*M*" -g -a | then do perf report -n -stdio
          -  eBPF (sandbox)
             -  [ebpf tools](../pictures/ebpf_tools.png)
             -  biosnoop
          - Processor Analysis
            - [CPU Utilization meaning](../pictures/CPU_Utilization_Calculation.png)
            - waiting on stalled cycles
            - IPC: instruction per cycle, range(0.x, x), Netflix says if < 0.2, stalled on memory
            - msrs (model specific registers)
              - showboost -- check the clock rate
          - Memory
            - tlbstat -C0 1
          - [CPU tools take aways](../pictures/cpu-perf-tool.png)
  - Velocity

# Improvement Method
## Benchmarking
## Profiling
## Tracing
- [linux tracing events](../pictures/Linux_tracing_events.png)
  - Offline CPU Tracing
## Tuning
- [Linux tuning settings](../pictures/normal_tuning_settings.png)
- [Linux tuning settings2](../pictures/linux_tuning_settings_2.png)
## Load Increases
- Auto Scaling
  - based on load average CPU utilization latency;
- Bad Push
  - traffic managed by Load balancers, do canary roll tests automatically
- INstance Failure
  - latency and fault tolerance for dependency services
- Disaster Recovery
  - region level failure fault tolerance
    - simulation: Chaos engineering