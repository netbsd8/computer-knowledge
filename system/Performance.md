# Metrics
## CPU utilization
- per min -> per sec -> hitmap (each col a second, each row may be several tens of micrsec, color dense show CPU usage at that time period)
    - perf: Linux profiler (sampling)
      - perf record -F99 -a
    - CPU flame graph (via perf): a visualization of profiled stack traces and what are the precentage that the different code functions are doing
      - x-axis: population of the samples; y-axis: function
      - [flame graph](../pictures/CPU-flame-graph-implementation.png)

- Root Cause
  - Basic Steps:
    - [uptime](../pictures/uptime_output.png)
    - [top](../pictures/top_output.png)
    - [vmstat](../pictures/vmstat_output.png)
    - [iostat](../pictures/iostat_output.png)
    - [free](../pictures/free_output.png)
    - [strace](../pictures/strace_output.png)
    - [tcpdump](../pictures/tcpdump_output.png)
    - [nstat](../pictures/nstat_output.png)
    - [slabtop](../pictures/slabtop_output.png)
    - [pcstat](../pictures/pcstat_output.png)
    - [docker stats](../pictures/docker_stats_output.png)
    - [showboost](../pictures/showboost_output.png)
    - [other static performance tuning tools](../pictures/static_performance_tunning_tools.png)
  - Observability
    - [tools](../pictures/Linux_components_performance_tools.png) vs htop with colors
      - sampling, so it may miss short-run processes; using atop instead
  - Methodology
    - Service Level:
      - Resource Analysis
      - RED method
        - Request Rate
        - Errors
        - Duration
      - Metric and event correlations
        - tracing
      - Latency Drilldowns
    - Instance Level:
      - Log Analysis
      - Micro-benchmarking
      - Drill-down analysis
      - Workload Characterization
        - Who: which pids, programs, users
        - Why: code paths, context
        - What: CPU instructions, cycles
        - How: changing over time
      - USE method
        - Utilization
        - Saturation --> queue
          - if CPU is saturated (high CPU utilization, there may be competition for the CPU caches)
        - Errors
      - [linux perf analysis in 60s](../pictures/Linux_perf_analysis_in_60s.png)
        - uptime --> load averages
        - dmesg -T | tail --> kernel errors
        - vmstat 1 --> overall stats by time
        - mpstat -P ALL 1 --> CPU balance
        - pidstat 1 --> process usage
        - iostat -xz 1 --> disk I/O
        - free -m --> memory usage
        - sar -n DEV 1 --> network I/O
        - sar -n TCP, ETCP 1 --> TCP stats
        - top --> check overview
      - [Linux performance Tool](../pictures/Linux_components_performance_tools.png)
        - Container ware tool (cgroup supporting)
        - Statistics
          - vmstat, pidstat, iostat, 
            - [mpstat](../pictures/mpstat_output.png), 
            - [pmcarch](../pictures/pmcarch_output.png) --> intel performance counters
              - IPC metrics --> it should be tuned to >= 1.5
              - LLC: L3 cache hit rate
              - [context switch](../pictures/perf_context_switch.png): 
              - [cpudist](../pictures/bpf_cpudist_process.png)
                - high value will clear the L1 cache, put more pressure on LC cache and drive the hit rate down
                - high value will prevent application from having enough time to warm up the CPU cache
            - sar
        - Profiling
          - CPU flame graph
        - Tracing
          - [tracing stack](../pictures/Linux_tracing_stack.png)
          -  ftrace
          -  perf
             -  iolatency (hardware)
             -  iosnoop (application)
             -  example: perf record -e block:block_rq_issue --filter rwbs ~ "*M*" -g -a | then do perf report -n -stdio
          -  eBPF (sandbox)
             -  [ebpf tools](../pictures/ebpf_tools.png)
             -  biosnoop
          - Processor Analysis
            - Clock rate
            - [Interconnects](../pictures/CPU_Interconnection.png) (waiting there be counted into CPU Utilization)
            - Special instructions: AVX-512 Vector Neural Network Instructions
            - Multi-socket
            - [SMT](../pictures/SMT_hardware_thread.png) (simultaneous multithreading / hardware threads)
            - [P-cores and E-cores](../pictures/P-cores_E-cores.png)
            - Accelerators:
              - GPUs
              - FPGAs
              - IPUs, DPUs, TPUs, etc.
            - [CPU Utilization meaning](../pictures/CPU_Utilization_Calculation.png)
              - waiting on stalled cycles
              - IPC: instruction per cycle, range(0.x, x), Netflix says if < 0.2, stalled on memory
            - msrs (model specific registers)
              - showboost -- check the clock rate
          - Memory
            - pmcarch (performance monitoring counters) --> IPC
            - DDR5 memory 
              - bandwidth: 51.2 GB/s
              - Latency: 14.38 ns (memory clock: 200MHZ)
              - HBM: high bandwidth memory (HBM only server (package CPU with HBM memory))
            - Server DRAM size: up to 4TB DDR-4
            - tlbstat -C0 1
          - Disks
            - rotational disk
              - 550MB/s
            - flash memory-based disks
              - SSD: 13GB/s
            - Storage interconnects
              - SAS-4 cards
              - PCIe 5.0: 63GB/s with 16 lane
              - NVme 1.4: bandwidth bounded by PCIe bus
          - Networking
            - Hardware:
              - 400 Gb/s switches/routers
              - NIC: kTLS(TLS offload to the NIC)
              - FPGA, ePBF supports
            - Protocols:
              - QUIC/HTTP/3
                - TCP-like sessions over (fast) UDP.
                - 0-RTT connection handshakes.
              - MP-TCP
                - multipath TCP. Use multiple paths in parallel to imporve throughput and reliability (Linux 5.6)
              - TCP Congestion control Algorithm
                - DCTCP (data center TCP -- Linux 3.18)
                - TCP NV (new vegas -- Linux 4.8)
                - tCP BBR (Bottleneck Bandwidth and RTT improvement)
              - Linux Networking
                - [queues](../pictures/Network_Stack_Queues.png)
                - [sending path](../pictures/Linux_network_send_path.png)
                - XDP (eXpress Data Path): a role previously served by DPDK and kernel bypass.
          - Kernel
            - [io_uring](../pictures/io_uring_01.png) -- faster syscalls using shared buffers and allow I/O to be batched and async 
            - eBPF
          - Hypervisors
            - Containers:
              - Cgroup v2
              - container scheduler adoption
              - perf tools still not container aware: 
            - [Hardware](../pictures/Hypervisors_hardware_based.png) 
              - Xen
              - KVM
              - Nitro
          - [CPU tools take aways](../pictures/cpu-perf-tool.png)
  - Velocity

# Improvement Method
## Benchmarking
## Profiling
## Tracing
- [linux tracing events](../pictures/Linux_tracing_events.png)
  - Offline CPU Tracing
## Tuning
- [Linux tuning settings](../pictures/normal_tuning_settings.png)
- [Linux tuning settings2](../pictures/linux_tuning_settings_2.png)
## Load Increases
- Auto Scaling
  - based on load average CPU utilization latency;
- Bad Push
  - traffic managed by Load balancers, do canary roll tests automatically
- INstance Failure
  - latency and fault tolerance for dependency services
- Disaster Recovery
  - region level failure fault tolerance
    - simulation: Chaos engineering